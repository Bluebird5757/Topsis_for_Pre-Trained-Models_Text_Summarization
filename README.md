# üìö Text Summarization Model Selection using TOPSIS

## üöÄ Overview
This project applies the **TOPSIS (Technique for Order of Preference by Similarity to Ideal Solution)** method to determine the **best pre-trained model** for **text summarization**.

Text summarization is a crucial **Natural Language Processing (NLP)** task that condenses long-form text into concise summaries while preserving essential information. Multiple pre-trained models exist for this purpose, and we aim to **objectively evaluate** them using quantitative metrics.

---

## üõ† Methodology

### 1Ô∏è‚É£ Selected Pre-Trained Models
We compare the following **transformer-based models**:
- `allenai/led-base-16384`
- `facebook/bart-large-cnn`
- `t5-large`
- `t5-small`
- `google/pegasus-xsum`
- `facebook/mbart-large-50`

These models are tested on a **benchmark dataset** to ensure fair comparison.

---

### 2Ô∏è‚É£ Dataset Used
The dataset chosen for this evaluation is **CNN/DailyMail**, a widely used dataset for abstractive summarization.

> üìå **Dataset Source:** `cnn_dailymail` from Hugging Face `datasets` library

---

### 3Ô∏è‚É£ Evaluation Metrics
Each summarization model is evaluated based on the following criteria:

| **Metric**         | **Description**                                      | **Preference** |
|-------------------|------------------------------------------------|-------------|
| **ROUGE-1**       | Measures overlap of unigrams between generated and reference summaries | Higher is better |
| **ROUGE-2**       | Measures overlap of bigrams (two-word sequences) | Higher is better |
| **ROUGE-L**       | Measures longest common subsequence overlap | Higher is better |
| **BLEU**          | Measures the precision of generated summaries based on n-gram matches | Higher is better |
| **Inference Time** | Time taken by the model to generate a summary | Lower is better |

---

### 4Ô∏è‚É£ Applying TOPSIS for Model Ranking
The **TOPSIS algorithm** follows these steps to rank the models:

1. **Normalize the dataset** to ensure all metrics are on the same scale.
2. **Determine the ideal best and worst solutions**:
   - **Best Solution:** Highest ROUGE and BLEU scores, lowest inference time.
   - **Worst Solution:** Lowest ROUGE and BLEU scores, highest inference time.
3. **Compute Euclidean distance** of each model from the ideal best and worst.
4. **Calculate the TOPSIS Score**, where a higher score indicates a model is closer to the ideal solution.
5. **Rank the models** based on their TOPSIS scores.

---

### üìà Visualizations
The following plots provide insights into the performance of different models.

#### 1Ô∏è‚É£ ROUGE and BLEU Score Comparison
![rouge_bleu_scores](https://github.com/user-attachments/assets/your_generated_graph_link1)

#### 2Ô∏è‚É£ TOPSIS Score Ranking
![topsis_scores](https://github.com/user-attachments/assets/your_generated_graph_link2)

---

### üìä Results
The models are ranked based on their **TOPSIS** Score.

| Model                | ROUGE-1 | ROUGE-2 | ROUGE-L | BLEU  | Inference Time | TOPSIS Score | Rank |
|----------------------|---------|---------|---------|------|---------------|--------------|------|
| LED                 | 0.82    | 0.80    | 0.79    | 0.60 | 1.20          | 0.79         | 1    |
| BART                | 0.62    | 0.55    | 0.58    | 0.35 | 1.50          | 0.70         | 2    |
| T5-Large            | 0.67    | 0.65    | 0.63    | 0.25 | 1.80          | 0.53         | 3    |
| T5-Small            | 0.55    | 0.50    | 0.52    | 0.18 | 2.10          | 0.28         | 4    |
| Pegasus            | 0.45    | 0.40    | 0.38    | 0.12 | 2.30          | 0.21         | 5    |
| mBART              | 0.52    | 0.48    | 0.47    | 0.20 | 2.50          | 0.20         | 6    |

üèÜ The model ranked **1st** is the best model for summarization based on TOPSIS!

